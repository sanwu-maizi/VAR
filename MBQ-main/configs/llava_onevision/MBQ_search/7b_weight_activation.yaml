# model args
model: llava_onevision
model_args: pretrained="lmms-lab/llava-onevision-qwen2-7b-ov"

# calibration data args
calib_data: coco
n_samples: 128
data_path: "your/coco/caption/path"
image_folder: "your/coco/image/folder"
few_shot_format: False # if True, concat two samples to simulate few shot
interleave_format: False # if True, insert pure text data between two image-text pairs to simulate interleave data
text_data_path: "" # You should specify this arg if you want to use interleave_format 

# quantization config
method: mbq
run_process: True
w_bit: 4
a_bit: 8
distort: False # if True, use distort feature map during scale search process
reweight: True # if True, use avg gradient to reweight the loss of each modality
loss_mode: mae 

# output args
scale_path: scale_cache/mbq/llava_ov_7b_w4a8.pt


